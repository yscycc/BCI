{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NvpSH85JEuIg"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39415,"status":"ok","timestamp":1713311745724,"user":{"displayName":"Ted Yang","userId":"14430424094859794716"},"user_tz":240},"id":"YSTnmUoKQ3qW","outputId":"52abc6e3-b043-4e50-aa72-39e1f5c7face"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting catboost\n","  Downloading catboost-1.2.3-cp310-cp310-manylinux2014_x86_64.whl (98.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.1.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n","Installing collected packages: catboost\n","Successfully installed catboost-1.2.3\n"]}],"source":["#Set up the notebook environment\n","!pip install catboost xgboost lightgbm scikit-learn\n","import catboost\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import scipy.io\n","from scipy import stats\n","from scipy.stats import pearsonr\n","from scipy import signal as sig\n","from scipy.signal import butter, filtfilt, welch, decimate\n","from scipy.interpolate import CubicSpline\n","from tqdm import tqdm\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n","from xgboost import XGBRegressor\n","from catboost import CatBoostRegressor\n","from lightgbm import LGBMRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZIBeObTJtMcL"},"outputs":[],"source":["data_train = scipy.io.loadmat('/content/drive/Shareddrives/BE 5210 Shared Drive/Ted Yang/Copy of raw_training_data.mat')\n","data_leader = scipy.io.loadmat('/content/drive/Shareddrives/BE 5210 Shared Drive/Ted Yang/Copy of leaderboard_data.mat')"]},{"cell_type":"code","source":["data_new_1 = scipy.io.loadmat('/content/drive/Shareddrives/BE 5210 Shared Drive/additional data/BCICIV_4_mat/sub1_comp.mat')\n","data_new_2 = scipy.io.loadmat('/content/drive/Shareddrives/BE 5210 Shared Drive/additional data/BCICIV_4_mat/sub2_comp.mat')\n","data_new_3 = scipy.io.loadmat('/content/drive/Shareddrives/BE 5210 Shared Drive/additional data/BCICIV_4_mat/sub3_comp.mat')"],"metadata":{"id":"uJbj2XMqcJ8Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UF40j08Z4h6u"},"source":["* Data Structure\n","* 6min, 40s, 400,000 samples, 1000Hz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVtjEoQBu1F5"},"outputs":[],"source":["data_leader = data_leader['leaderboard_ecog']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713311755447,"user":{"displayName":"Ted Yang","userId":"14430424094859794716"},"user_tz":240},"id":"fffCu7SuEorm","outputId":"bff53606-4198-47de-e632-75f75ad9bc41"},"outputs":[{"output_type":"stream","name":"stdout","text":["The total number of samples is 300000.\n","The number of samples in the training set is 210000.\n","The number of samples in the testing set is 90000.\n"]}],"source":["data_glove = data_train['train_dg']\n","data_ecog = data_train['train_ecog']\n","train_dg = []\n","test_dg = []\n","train_ecog = []\n","test_ecog = []\n","\n","# Define the split ratio (e.g., 70% for training and 30% for testing)\n","split_ratio = 0.7\n","\n","# Function to split each subject's data\n","def split_data(data, split_ratio):\n","    num_samples = len(data[0])\n","    split_point = int(num_samples * split_ratio)\n","    train = data[0][:split_point]\n","    test = data[0][split_point:]\n","    return train, test\n","\n","# Loop over each subject's data for glove and ECoG data\n","for glove_data, ecog_data in zip(data_glove, data_ecog):\n","    train_dg_split, test_dg_split = split_data(glove_data, split_ratio)\n","    train_ecog_split, test_ecog_split = split_data(ecog_data, split_ratio)\n","\n","    # Append the split data to their respective lists\n","    train_dg.append(train_dg_split)\n","    test_dg.append(test_dg_split)\n","    train_ecog.append(train_ecog_split)\n","    test_ecog.append(test_ecog_split)\n","\n","# Now train_dg, test_dg, train_ecog, and test_ecog contain the split data for each subject\n","\n","#training set\n","total_samples_train_ecog = len(train_ecog[0])\n","\n","#testing set\n","total_samples_test_ecog = len(test_ecog[0])\n","\n","#full dataset sample before the split\n","total_samples_full_ecog = total_samples_train_ecog + total_samples_test_ecog\n","\n","print(f'The total number of samples is {total_samples_full_ecog}.')\n","print(f'The number of samples in the training set is {total_samples_train_ecog}.')\n","print(f'The number of samples in the testing set is {total_samples_test_ecog}.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t50kkJOjtSC7"},"outputs":[],"source":["def filter_data(raw_eeg, fs=1000):\n","    \"\"\"\n","    Input:\n","      raw_eeg (samples x channels): the raw signal\n","      fs: the sampling rate (1000 for this dataset)\n","\n","    Output:\n","      clean_data (samples x channels): the filtered signal\n","    \"\"\"\n","\n","    #lowcut and highcut for the bandpass filter\n","    lowcut = 0.15\n","    highcut = 200.0\n","\n","    #Butterworth bandpass filter\n","    nyquist = 0.5 * fs\n","    low = lowcut / nyquist\n","    high = highcut / nyquist\n","    b, a = butter(4, [low, high], btype='band', analog=False)\n","\n","    #apply filter to raw ECoG data\n","    clean_data = filtfilt(b, a, raw_eeg, axis=0)\n","\n","    return clean_data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713311755448,"user":{"displayName":"Ted Yang","userId":"14430424094859794716"},"user_tz":240},"id":"dg6ExOGJPghf","outputId":"69de0ef3-cc0e-4f52-f08d-54f92d10e873"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 5999 windows using all the data.\n","There are 4199 windows training data.\n","There are 1799 windows testing data.\n"]}],"source":["#window and step sizes\n","window_length = 100\n","step_size = 50\n","\n","#number of feature windows M for the full dataset, training, and testing sets\n","M_full = 1 + (total_samples_full_ecog - window_length) // step_size\n","M_train = 1 + (total_samples_train_ecog - window_length) // step_size\n","M_test = 1 + (total_samples_test_ecog - window_length) // step_size\n","\n","print(f'There are {M_full} windows using all the data.')\n","print(f'There are {M_train} windows training data.')\n","print(f'There are {M_test} windows testing data.')"]},{"cell_type":"markdown","metadata":{"id":"8EjVCGNjuVod"},"source":["# **Feature Extraction**"]},{"cell_type":"markdown","metadata":{"id":"H8joIA_wuopR"},"source":["# This one has got 0.4426 for testing set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1713289570256,"user":{"displayName":"Ted Yang","userId":"14430424094859794716"},"user_tz":240},"id":"_dhnPBb0QI9c","outputId":"37979bff-3aa0-4778-c093-e369cb9627e4"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"#This one has got 0.4426 for testing set\\n\\ndef get_features(filtered_window, fs=1000):\\n\\n    Inputs:\\n        filtered_window (window_samples x channels): the window of the filtered ECoG signal\\n        fs: sampling rate\\n\\n    Output:\\n        features (channels x num_features): the features calculated on each channel for the window\\n\\n    # Make sure nperseg does not exceed window size\\n    nperseg = min(filtered_window.shape[0], 256)\\n\\n    # Basic statistical features\\n    mean = np.mean(filtered_window, axis=0)\\n    variance = np.var(filtered_window, axis=0)\\n    lmp = np.mean(np.abs(filtered_window), axis=0)  # Local Motor Potential\\n\\n    # Compute the Power Spectral Density (PSD) for each channel\\n    freqs, psd = welch(filtered_window.T, fs, nperseg=nperseg)\\n\\n    # Define frequency bands\\n    bands = {\\n        'alpha': (8, 12),\\n        #'beta': (13, 30),\\n        #'theta': (4, 7),  # Adding an extra band as an example\\n        #'low_gamma': (30, 50),  # Another example band\\n        'custom_band1': (18, 24),\\n        'custom_band2': (75, 115),\\n        'custom_band3': (125, 159),\\n        'custom_band4': (159, 175)\\n    }\\n\\n    # Initialize a dictionary to hold the power for each band\\n    band_powers = {band: np.zeros(psd.shape[0]) for band in bands}\\n\\n    # Calculate power for each band\\n    for band, (low, high) in bands.items():\\n        freq_mask = (freqs >= low) & (freqs <= high)\\n        band_powers[band] = np.sum(psd[:, freq_mask], axis=1)\\n\\n    # Collect all features\\n    all_features = [mean, variance, lmp] + list(band_powers.values())\\n    features = np.vstack(all_features).T\\n\\n    return features\\n\""]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"#This one has got 0.4426 for testing set\n","\n","def get_features(filtered_window, fs=1000):\n","\n","    Inputs:\n","        filtered_window (window_samples x channels): the window of the filtered ECoG signal\n","        fs: sampling rate\n","\n","    Output:\n","        features (channels x num_features): the features calculated on each channel for the window\n","\n","    # Make sure nperseg does not exceed window size\n","    nperseg = min(filtered_window.shape[0], 256)\n","\n","    # Basic statistical features\n","    mean = np.mean(filtered_window, axis=0)\n","    variance = np.var(filtered_window, axis=0)\n","    lmp = np.mean(np.abs(filtered_window), axis=0)  # Local Motor Potential\n","\n","    # Compute the Power Spectral Density (PSD) for each channel\n","    freqs, psd = welch(filtered_window.T, fs, nperseg=nperseg)\n","\n","    # Define frequency bands\n","    bands = {\n","        'alpha': (8, 12),\n","        #'beta': (13, 30),\n","        #'theta': (4, 7),  # Adding an extra band as an example\n","        #'low_gamma': (30, 50),  # Another example band\n","        'custom_band1': (18, 24),\n","        'custom_band2': (75, 115),\n","        'custom_band3': (125, 159),\n","        'custom_band4': (159, 175)\n","    }\n","\n","    # Initialize a dictionary to hold the power for each band\n","    band_powers = {band: np.zeros(psd.shape[0]) for band in bands}\n","\n","    # Calculate power for each band\n","    for band, (low, high) in bands.items():\n","        freq_mask = (freqs >= low) & (freqs <= high)\n","        band_powers[band] = np.sum(psd[:, freq_mask], axis=1)\n","\n","    # Collect all features\n","    all_features = [mean, variance, lmp] + list(band_powers.values())\n","    features = np.vstack(all_features).T\n","\n","    return features\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0LEHJyv9sBOx"},"outputs":[],"source":["def get_features(filtered_window, fs=1000):\n","    \"\"\"\n","    Inputs:\n","        filtered_window (window_samples x channels): the window of the filtered ECoG signal\n","        fs: sampling rate\n","\n","    Output:\n","        features (channels x num_features): the features calculated on each channel for the window\n","    \"\"\"\n","    # Make sure nperseg does not exceed window size\n","    nperseg = min(filtered_window.shape[0], 256)\n","\n","    # Basic statistical features\n","    mean = np.mean(filtered_window, axis=0)\n","    variance = np.var(filtered_window, axis=0)\n","    lmp = np.mean(np.abs(filtered_window), axis=0)  # Local Motor Potential\n","\n","    # Compute the Power Spectral Density (PSD) for each channel\n","    freqs, psd = welch(filtered_window.T, fs, nperseg=nperseg)\n","\n","    # Define frequency bands\n","    bands = {\n","        'alpha': (8, 12),\n","        #'beta': (13, 30),\n","        #'theta': (4, 7),  # Adding an extra band as an example\n","        #'low_gamma': (30, 50),  # Another example band\n","        'custom_band1': (18, 24),\n","        'custom_band2': (75, 115),\n","        'custom_band3': (125, 159),\n","        'custom_band4': (159, 175)\n","    }\n","\n","    # Initialize a dictionary to hold the power for each band\n","    band_powers = {band: np.zeros(psd.shape[0]) for band in bands}\n","\n","    # Calculate power for each band\n","    for band, (low, high) in bands.items():\n","        freq_mask = (freqs >= low) & (freqs <= high)\n","        band_powers[band] = np.sum(psd[:, freq_mask], axis=1)\n","\n","    # Collect all features\n","    all_features = [mean, variance, lmp] + list(band_powers.values())\n","    features = np.vstack(all_features).T\n","\n","    return features"]},{"cell_type":"markdown","metadata":{"id":"4IfflHEXucWV"},"source":["\n","\n","---\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6K9u38WEuKmH"},"outputs":[],"source":["def get_windowed_feats(raw_ecog, fs, window_length_ms, window_overlap_ms):\n","    \"\"\"\n","    Inputs:\n","        raw_ecog (samples x channels): the raw signal\n","        fs: the sampling rate (1000 for this dataset)\n","        window_length_ms: the window's length in milliseconds\n","        window_overlap_ms: the window's overlap in milliseconds\n","\n","    Output:\n","        all_feats (num_windows x (channels x features)): the features for each channel for each time window\n","    \"\"\"\n","    window_length = int(fs * (window_length_ms / 1000))\n","    window_overlap = int(fs * (window_overlap_ms / 1000))\n","    step_size = window_length - window_overlap\n","\n","    #filter_data function placeholder\n","    filtered_ecog = raw_ecog\n","\n","    total_samples = raw_ecog.shape[0]\n","    num_windows = 1 + (total_samples - window_length) // step_size\n","\n","    feature_list = []\n","\n","    for start_idx in tqdm(range(0, total_samples - window_length + 1, step_size)):\n","        end_idx = start_idx + window_length\n","        window = filtered_ecog[start_idx:end_idx, :]\n","\n","        #features for this window\n","        window_features = get_features(window, fs)\n","\n","        #append features to list\n","        feature_list.append(window_features.flatten())\n","\n","    #stack to get 2D array (num_windows x features)\n","    all_feats = np.array(feature_list)\n","\n","    return all_feats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJ9x2oWPQQ1a"},"outputs":[],"source":["def create_R_matrix(features, N=3):\n","    \"\"\"\n","    Input:\n","        features: The 2D array of features (num_windows x num_features_per_window).\n","        N: The number of previous windows to include for each prediction instance.\n","\n","    Output:\n","        R: The response matrix.\n","    \"\"\"\n","    #first N-1 rows to beginning of features matrix\n","    adjusted_features = np.vstack([features[:N-1], features])\n","\n","    #number of total instances after adjustment\n","    M_prime = adjusted_features.shape[0] - (N - 1)\n","\n","    #initialize response matrix R\n","    num_features = features.shape[1]\n","    R = np.zeros((M_prime, N * num_features + 1))  # +1 for the intercept term\n","\n","    #fill in R\n","    for i in range(M_prime):\n","        #extract N consecutive windows of features\n","        consecutive_features = adjusted_features[i:i+N].flatten()\n","        #fill corresponding row in R, adding 1 as the last column\n","        R[i, 1:] = consecutive_features\n","        R[i, 0] = 1  # Intercept term\n","\n","    return R"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DUt1K4YMQXsG"},"outputs":[],"source":["train_ecog_1 = train_ecog[0]\n","train_ecog_2 = train_ecog[1]\n","train_ecog_3 = train_ecog[2]\n","\n","train_glove_1 = train_dg[0]\n","train_glove_2 = train_dg[1]\n","train_glove_3 = train_dg[2]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22117,"status":"ok","timestamp":1713311777776,"user":{"displayName":"Ted Yang","userId":"14430424094859794716"},"user_tz":240},"id":"cnCpl5FgQdea","outputId":"c964c5ad-b903-4dad-c4b3-53792d4d4f3c"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4199/4199 [00:07<00:00, 528.53it/s]\n","100%|██████████| 4199/4199 [00:05<00:00, 820.77it/s]\n","100%|██████████| 4199/4199 [00:08<00:00, 468.57it/s] \n"]},{"output_type":"execute_result","data":{"text/plain":["4199"]},"metadata":{},"execution_count":11}],"source":["#feature extraction for each subject\n","feature_1 = get_windowed_feats(train_ecog_1, 1000, 100, 50)\n","feature_2 = get_windowed_feats(train_ecog_2, 1000, 100, 50)\n","feature_3 = get_windowed_feats(train_ecog_3, 1000, 100, 50)\n","\n","num_target_windows = feature_1.shape[0]\n","num_target_windows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFIJ7I2OQmSq"},"outputs":[],"source":["#get R matrix\n","R_1 = create_R_matrix(feature_1, N=3)\n","R_2 = create_R_matrix(feature_2, N=3)\n","R_3 = create_R_matrix(feature_3, N=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AclgrYVgRCoY"},"outputs":[],"source":["def get_windowed_target(glove_data, window_length_ms, window_overlap_ms, fs=1000):\n","    window_length = int(fs * (window_length_ms / 1000))\n","    window_overlap = int(fs * (window_overlap_ms / 1000))\n","    step_size = window_length - window_overlap\n","\n","    num_windows = 1 + (glove_data.shape[0] - window_length) // step_size\n","    targets = []\n","\n","    for start_idx in range(0, glove_data.shape[0] - window_length + 1, step_size):\n","        end_idx = start_idx + window_length\n","        window = glove_data[start_idx:end_idx, :]\n","        targets.append(np.mean(window, axis=0))  # Example: mean over the window\n","\n","    return np.array(targets)\n","\n","# Windowing target glove data\n","target_1 = get_windowed_target(train_glove_1, 100, 50)\n","target_2 = get_windowed_target(train_glove_2, 100, 50)\n","target_3 = get_windowed_target(train_glove_3, 100, 50)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PlBsAvCDt1i2"},"outputs":[],"source":["# Define parameter distributions for XGBoost\n","\"\"\"xgb_params = {\n","    'n_estimators': np.arange(100, 501, 50),\n","    'max_depth': np.arange(3, 10, 1),\n","    'learning_rate': np.linspace(0.01, 0.3, 10),\n","    'subsample': np.linspace(0.5, 1, 6),\n","    'colsample_bytree': np.linspace(0.5, 1, 6)\n","}\"\"\"\n","\n","# Define parameter distributions for CatBoost\n","cat_params = {\n","    'iterations': np.arange(100, 501, 50),\n","    'depth': np.arange(3, 10, 1),\n","    'learning_rate': np.linspace(0.01, 0.3, 10),\n","    'l2_leaf_reg': np.linspace(1, 10, 10)\n","}\n","\n","# Setup RandomizedSearchCV for XGBoost\n","\"\"\"xgb_random = RandomizedSearchCV(\n","    XGBRegressor(),\n","    param_distributions=xgb_params,\n","    n_iter=100,\n","    scoring='neg_mean_squared_error',\n","    cv=3,\n","    verbose=1,\n","    random_state=42,\n","    n_jobs=-1\n",")\"\"\"\n","\n","# Setup RandomizedSearchCV for CatBoost\n","cat_random = RandomizedSearchCV(\n","    CatBoostRegressor(verbose=False),\n","    param_distributions=cat_params,\n","    n_iter=100,\n","    scoring='neg_mean_squared_error',\n","    cv=3,\n","    verbose=1,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","# Fit the models (example for one dataset)\n","xgb_random.fit(R_1[:, 1:], target_1)\n","cat_random.fit(R_1[:, 1:], target_1)\n","\n","# Print the best parameters and scores\n","print(\"Best parameters for XGBoost:\", xgb_random.best_params_)\n","print(\"Best score for XGBoost:\", xgb_random.best_score_)\n","print(\"Best parameters for CatBoost:\", cat_random.best_params_)\n","print(\"Best score for CatBoost:\", cat_random.best_score_)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ns8IuqhSRFuM","outputId":"d05dcfde-5097-426b-b3a5-8e1dab9bb6de"},"outputs":[{"data":{"text/plain":["<catboost.core.CatBoostRegressor at 0x7a250b187460>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# XGBoost Model\n","xgb_model1 = XGBRegressor(n_estimators=300, max_depth=5, learning_rate=0.1)\n","xgb_model1.fit(R_1[:, 1:], target_1)\n","\n","xgb_model2 = XGBRegressor(n_estimators=300, max_depth=5, learning_rate=0.1)\n","xgb_model2.fit(R_2[:, 1:], target_2)\n","\n","xgb_model3 = XGBRegressor(n_estimators=300, max_depth=5, learning_rate=0.1)\n","xgb_model3.fit(R_3[:, 1:], target_3)\n","\n","# CatBoost Model\n","cat_model1 = CatBoostRegressor(iterations=300, depth=5, learning_rate=0.1, loss_function='MultiRMSE', verbose=False)\n","cat_model1.fit(R_1[:, 1:], target_1)\n","\n","cat_model2 = CatBoostRegressor(iterations=300, depth=5, learning_rate=0.1, loss_function='MultiRMSE', verbose=False)\n","cat_model2.fit(R_2[:, 1:], target_2)\n","\n","cat_model3 = CatBoostRegressor(iterations=300, depth=5, learning_rate=0.1, loss_function='MultiRMSE', verbose=False)\n","cat_model3.fit(R_3[:, 1:], target_3)"]},{"cell_type":"code","source":["# CatBoost Model\n","cat_model1 = CatBoostRegressor(iterations=500, depth=5, learning_rate=0.1, loss_function='MultiRMSE', verbose=False)\n","cat_model1.fit(R_1[:, 1:], target_1)\n","\n","cat_model2 = CatBoostRegressor(iterations=500, depth=5, learning_rate=0.1, loss_function='MultiRMSE', verbose=False)\n","cat_model2.fit(R_2[:, 1:], target_2)\n","\n","cat_model3 = CatBoostRegressor(iterations=500, depth=5, learning_rate=0.1, loss_function='MultiRMSE', verbose=False)\n","cat_model3.fit(R_3[:, 1:], target_3)"],"metadata":{"id":"xw-AiUKfS9YP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"38DscRqIXaZ9"},"source":["----> Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1713291208982,"user":{"displayName":"Ted Yang","userId":"14430424094859794716"},"user_tz":240},"id":"aioQh2bGWooq","outputId":"b1141bca-2a01-43b8-e1a8-0e6d4f98e5dd"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"#create linear f function\\ndef linear_filter(R_matrix, train_data):\\n    num_samples_features = R_matrix.shape[0]\\n    num_samples_flexion = train_data.shape[0]\\n\\n    #downsample train_data if it doesn't match the number of feature windows in R_matrix\\n    if num_samples_features != num_samples_flexion:\\n        downsample_factor = num_samples_flexion // num_samples_features\\n\\n        #make nsure downsample_factor at least 1\\n        downsample_factor = max(downsample_factor, 1)\\n        downsize_data = decimate(train_data, downsample_factor, axis=0)\\n\\n        #make sure downsize_data has same number of samples as R_matrix\\n        if downsize_data.shape[0] > num_samples_features:\\n            downsize_data = downsize_data[:num_samples_features]\\n    else:\\n        downsize_data = train_data\\n\\n    #linear filter f using matrix multiplication\\n    #p1 = (R^T R)^(-1)\\n    p1 = np.linalg.inv(np.matmul(R_matrix.T, R_matrix))\\n\\n    #p2 = R^T Y\\n    p2 = np.matmul(R_matrix.T, downsize_data)\\n\\n    #f = p1 * p2\\n    lf = np.matmul(p1, p2)\\n\\n    return downsize_data, lf\""]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"#create linear f function\n","def linear_filter(R_matrix, train_data):\n","    num_samples_features = R_matrix.shape[0]\n","    num_samples_flexion = train_data.shape[0]\n","\n","    #downsample train_data if it doesn't match the number of feature windows in R_matrix\n","    if num_samples_features != num_samples_flexion:\n","        downsample_factor = num_samples_flexion // num_samples_features\n","\n","        #make nsure downsample_factor at least 1\n","        downsample_factor = max(downsample_factor, 1)\n","        downsize_data = decimate(train_data, downsample_factor, axis=0)\n","\n","        #make sure downsize_data has same number of samples as R_matrix\n","        if downsize_data.shape[0] > num_samples_features:\n","            downsize_data = downsize_data[:num_samples_features]\n","    else:\n","        downsize_data = train_data\n","\n","    #linear filter f using matrix multiplication\n","    #p1 = (R^T R)^(-1)\n","    p1 = np.linalg.inv(np.matmul(R_matrix.T, R_matrix))\n","\n","    #p2 = R^T Y\n","    p2 = np.matmul(R_matrix.T, downsize_data)\n","\n","    #f = p1 * p2\n","    lf = np.matmul(p1, p2)\n","\n","    return downsize_data, lf\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ML1QrWqjbBWb"},"outputs":[],"source":["def linear_filter(R_matrix, train_data, lambda_reg=0.01):\n","    num_samples_features = R_matrix.shape[0]\n","    num_samples_flexion = train_data.shape[0]\n","\n","    # Downsample train_data if it doesn't match the number of feature windows in R_matrix\n","    if num_samples_features != num_samples_flexion:\n","        downsample_factor = num_samples_flexion // num_samples_features\n","        downsample_factor = max(downsample_factor, 1)  # Ensure downsample_factor at least 1\n","        downsize_data = decimate(train_data, downsample_factor, axis=0)\n","\n","        # Ensure downsize_data has same number of samples as R_matrix\n","        if downsize_data.shape[0] > num_samples_features:\n","            downsize_data = downsize_data[:num_samples_features]\n","    else:\n","        downsize_data = train_data\n","\n","    # Regularized linear filter f using matrix multiplication\n","    # p1 = (R^T R + lambda * I)^(-1)\n","    lambda_identity = lambda_reg * np.eye(R_matrix.shape[1])  # Lambda times identity matrix\n","    p1 = np.linalg.inv(np.matmul(R_matrix.T, R_matrix) + lambda_identity)\n","\n","    # p2 = R^T Y\n","    p2 = np.matmul(R_matrix.T, downsize_data)\n","\n","    # f = p1 * p2\n","    lf = np.matmul(p1, p2)\n","\n","    return downsize_data, lf"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fWFVUOJMokvt"},"outputs":[],"source":["#assign test ecog & glove data\n","test_ecog_1 = test_ecog[0]\n","test_ecog_2 = test_ecog[1]\n","test_ecog_3 = test_ecog[2]\n","\n","test_glove_1 = test_dg[0]\n","test_glove_2 = test_dg[1]\n","test_glove_3 = test_dg[2]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IbyHoKlxAsL1","outputId":"d4d13bf9-d060-485e-85e6-6cc5443580c0"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1799/1799 [00:01<00:00, 1179.67it/s]\n","100%|██████████| 1799/1799 [00:01<00:00, 1663.87it/s]\n","100%|██████████| 1799/1799 [00:01<00:00, 1300.85it/s]\n"]}],"source":["#feature extraction of test set\n","test_feature_1 = get_windowed_feats(test_ecog_1, 1000, 100, 50)\n","test_feature_2 = get_windowed_feats(test_ecog_2, 1000, 100, 50)\n","test_feature_3 = get_windowed_feats(test_ecog_3, 1000, 100, 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"08QiaAwhoV33"},"outputs":[],"source":["#R matrix for test set\n","test_R_1 = create_R_matrix(test_feature_1, N=3)\n","test_R_2 = create_R_matrix(test_feature_2, N=3)\n","test_R_3 = create_R_matrix(test_feature_3, N=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":207,"status":"ok","timestamp":1713291222405,"user":{"displayName":"Ted Yang","userId":"14430424094859794716"},"user_tz":240},"id":"tcFiz168RJ0B","outputId":"5c391f2b-303d-4243-bbe9-0d2da7299e80"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'mse = mean_squared_error(target_1, ensemble_preds)\\nr2 = r2_score(target_1, ensemble_preds)\\nprint(\"MSE:\", mse)\\nprint(\"R^2:\", r2)'"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"mse = mean_squared_error(target_1, ensemble_preds)\n","r2 = r2_score(target_1, ensemble_preds)\n","print(\"MSE:\", mse)\n","print(\"R^2:\", r2)\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_cfdYYbPVLla"},"outputs":[],"source":["multi_preds_1 = lgb_multioutput1.predict(test_R_1[:, 1:])\n","multi_preds_2 = lgb_multioutput2.predict(test_R_2[:, 1:])\n","multi_preds_3 = lgb_multioutput3.predict(test_R_3[:, 1:])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"executionInfo":{"elapsed":250,"status":"error","timestamp":1713291223125,"user":{"displayName":"Ted Yang","userId":"14430424094859794716"},"user_tz":240},"id":"b68g91VZ7qb5","outputId":"8cf6a9b6-0fc9-4ee2-bb8b-579f6539e9d0"},"outputs":[{"ename":"AttributeError","evalue":"'list' object has no attribute 'predict'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-508a86559e19>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlgb_preds_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_model1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_R_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlgb_preds_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_model2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_R_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlgb_preds_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_model3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_R_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'predict'"]}],"source":["lgb_preds_1 = lgb_model1.predict(test_R_1[:, 1:])\n","lgb_preds_2 = lgb_model2.predict(test_R_2[:, 1:])\n","lgb_preds_3 = lgb_model3.predict(test_R_3[:, 1:])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"f7uJyxplYamJ","outputId":"0d54d2f3-2f30-41b5-ff8a-22ee00e588aa"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'lgb_preds_1 = lgb_model1.predict(test_R_1[:, 1:])\\nlgb_preds_2 = lgb_model2.predict(test_R_2[:, 1:])\\nlgb_preds_3 = lgb_model3.predict(test_R_3[:, 1:])'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["xgb_preds_1 = xgb_model1.predict(test_R_1[:, 1:])\n","xgb_preds_2 = xgb_model2.predict(test_R_2[:, 1:])\n","xgb_preds_3 = xgb_model3.predict(test_R_3[:, 1:])\n","\n","cat_preds_1 = cat_model1.predict(test_R_1[:, 1:])\n","cat_preds_2 = cat_model2.predict(test_R_2[:, 1:])\n","cat_preds_3 = cat_model3.predict(test_R_3[:, 1:])\n","\n","\"\"\"lgb_preds_1 = lgb_model1.predict(test_R_1[:, 1:])\n","lgb_preds_2 = lgb_model2.predict(test_R_2[:, 1:])\n","lgb_preds_3 = lgb_model3.predict(test_R_3[:, 1:])\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7YBCuSHpYSfs"},"outputs":[],"source":["#linear filter\n","test_glove_ds_1, tf_1 = linear_filter(test_R_1, test_glove_1)\n","test_glove_ds_2, tf_2 = linear_filter(test_R_2, test_glove_2)\n","test_glove_ds_3, tf_3 = linear_filter(test_R_3, test_glove_3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IUVilrOI6bzF"},"outputs":[],"source":["#generate prediction of linear f\n","y_pred_1 = np.dot(test_R_1, tf_1)\n","y_pred_2 = np.dot(test_R_2, tf_2)\n","y_pred_3 = np.dot(test_R_3, tf_3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IltKz2nOvwVv"},"outputs":[],"source":["#correlation using pearsonr\n","def calculate_correlations(y_true, y_pred):\n","    correlations = []\n","    for finger in range(y_true.shape[1]):  # Assuming y_true.shape[1] is the number of fingers\n","        corr, _ = pearsonr(y_true[:, finger], y_pred[:, finger])\n","        correlations.append(corr)\n","    return correlations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFWjqBwFv0s5"},"outputs":[],"source":["correlations_multi_1 = calculate_correlations(test_glove_ds_1, multi_preds_1)\n","correlations_multi_2 = calculate_correlations(test_glove_ds_2, multi_preds_2)\n","correlations_multi_3 = calculate_correlations(test_glove_ds_3, multi_preds_3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"H5U4Ku9jASkh"},"outputs":[],"source":["#correlations\n","correlations_xgb_1 = calculate_correlations(test_glove_ds_1, xgb_preds_1)\n","correlations_xgb_2 = calculate_correlations(test_glove_ds_2, xgb_preds_2)\n","correlations_xgb_3 = calculate_correlations(test_glove_ds_3, xgb_preds_3)\n","\n","correlations_cat_1 = calculate_correlations(test_glove_ds_1, cat_preds_1)\n","correlations_cat_2 = calculate_correlations(test_glove_ds_2, cat_preds_2)\n","correlations_cat_3 = calculate_correlations(test_glove_ds_3, cat_preds_3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ge5reMo7v9pT"},"outputs":[],"source":["correlations_multi_1, correlations_multi_2, correlations_multi_3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5ViPsNg72IT"},"outputs":[],"source":["#create function for correlation\n","def cor(test_data, f, R_matrix):\n","    \"\"\"\n","    Calculates Pearson correlation coefficients between true and predicted finger angles.\n","\n","    Parameters:\n","    - test_data: The actual finger flexion data for the test set (samples x fingers).\n","    - f: The linear filter coefficients (features x fingers) calculated from the training set.\n","    - R_matrix: The feature matrix from the test ECoG data (samples x features).\n","\n","    Returns:\n","    - A list of Pearson correlation coefficients, one for each finger.\n","    \"\"\"\n","    #predict finger angles\n","    predicted = np.dot(R_matrix, f)\n","\n","    #make sure predicted data has same shape as test data\n","    assert predicted.shape == test_data.shape, \"Shape mismatch between predicted and true data\"\n","\n","    #Pearson correlation for each finger\n","    subject_correlations = []\n","    for finger in range(test_data.shape[1]):\n","        true_angle = test_data[:, finger]\n","        predicted_angle = predicted[:, finger]\n","\n","        #correlation\n","        correlation = np.corrcoef(true_angle, predicted_angle)[0, 1]\n","\n","        #append correlation coefficient\n","        subject_correlations.append(correlation)\n","\n","    return subject_correlations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Qtxdlm9786R"},"outputs":[],"source":["#downsample glove data\n","train_glove_ds_1, f_1 = linear_filter(R_1, train_glove_1)\n","train_glove_ds_2, f_2 = linear_filter(R_2, train_glove_2)\n","train_glove_ds_3, f_3 = linear_filter(R_3, train_glove_3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBwa0j0h6hUD"},"outputs":[],"source":["\"\"\"correlations_lf_1, correlations_lf_2, correlations_lf_3\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GeasE5te74Al"},"outputs":[],"source":["\"\"\"#correlation\n","co1 = cor(train_glove_ds_1, f_1, R_1)\n","co2 = cor(train_glove_ds_2, f_2, R_2)\n","co3 = cor(train_glove_ds_3, f_3, R_3)\n","\n","(co1, co2, co3)\"\"\""]},{"cell_type":"markdown","metadata":{"id":"x-qUdlZas21L"},"source":["**XGBoost**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UQiW26qHbLOE","outputId":"ed1f3288-0e48-4c9b-9e0d-6ff16dd1d076"},"outputs":[{"data":{"text/plain":["([0.42232087362320864,\n","  0.5792825705548416,\n","  0.1375833511080457,\n","  0.4515971698723468,\n","  0.11280150058714171],\n"," [0.5085427665095934,\n","  0.17824688508863096,\n","  0.2125406013319665,\n","  0.5175084733145717,\n","  0.07650407718577555],\n"," [0.5698591901088298,\n","  0.3808595203763699,\n","  0.4531288289641911,\n","  0.5035554827206609,\n","  0.36204981936624525])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["(correlations_xgb_1, correlations_xgb_2, correlations_xgb_3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"66fGknGzEIDZ"},"outputs":[],"source":["\"\"\"correlations_xgb_m1, correlations_xgb_m2, correlations_xgb_m3\"\"\""]},{"cell_type":"markdown","metadata":{"id":"C1UW4xyds6Po"},"source":["**CatBoost**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qaAth2VPbPmC","outputId":"3a401eb7-a9ee-4590-d3f5-6892621d9902"},"outputs":[{"data":{"text/plain":["([0.4515281141722916,\n","  0.6100341432652044,\n","  0.17987936346454056,\n","  0.49870862185773174,\n","  0.11510920928297969],\n"," [0.49312217446906437,\n","  0.19973550922066757,\n","  0.28184504641469527,\n","  0.4870203260072296,\n","  0.33360122641518797],\n"," [0.5867612865384663,\n","  0.38391599391965514,\n","  0.516004225557485,\n","  0.5726448589428662,\n","  0.43779103958155086])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["(correlations_cat_1, correlations_cat_2, correlations_cat_3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mB_Xly13EBMd"},"outputs":[],"source":["\"\"\"correlations_cat_m1, correlations_cat_m2, correlations_cat_m3\"\"\""]},{"cell_type":"markdown","metadata":{"id":"UWFxrGC8tEQj"},"source":["**LGBoost**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"elapsed":303,"status":"error","timestamp":1713293849515,"user":{"displayName":"Ted Yang","userId":"14430424094859794716"},"user_tz":240},"id":"f4r1yVW9tCys","outputId":"868d20f9-6990-454a-8727-826192ed7b98"},"outputs":[{"ename":"NameError","evalue":"name 'correlations_lgb_1' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-f8f14b635bcd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mcorrelations_lgb_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelations_lgb_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelations_lgb_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'correlations_lgb_1' is not defined"]}],"source":["(correlations_lgb_1, correlations_lgb_2, correlations_lgb_3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Odzat-dR_C-Q"},"outputs":[],"source":["!jupyter nbconvert --to html final_project_part_2.ipynb"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}